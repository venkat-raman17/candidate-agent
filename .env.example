MCP_SERVER_URL=http://localhost:8081/mcp
MCP_CONNECT_TIMEOUT=30

# ── LLM: Anthropic (default) ─────────────────────────────────────────────────
# Required when LOCAL_LLM=false
ANTHROPIC_API_KEY=sk-ant-...
LLM_MODEL=claude-sonnet-4-6
LLM_TEMPERATURE=0.0

# ── LLM: Local (Ollama / LM Studio / vLLM) ───────────────────────────────────
# Set LOCAL_LLM=true to use a local OpenAI-compatible server instead of Anthropic.
# ANTHROPIC_API_KEY is not required in this mode.
#
# Ollama:
#   1. Install: https://ollama.com
#   2. Pull a model: ollama pull llama3.2
#   3. Set the vars below and LOCAL_LLM=true
#
# LM Studio: change LOCAL_LLM_BASE_URL to http://localhost:1234/v1
# vLLM:      change LOCAL_LLM_BASE_URL to http://localhost:8080/v1
LOCAL_LLM=true
LOCAL_LLM_BASE_URL=http://localhost:1234/v1
LOCAL_LLM_MODEL=openai/gpt-oss-20b
LOCAL_LLM_API_KEY=lmsudio

# ── Server ────────────────────────────────────────────────────────────────────
APP_HOST=0.0.0.0
APP_PORT=8000
LOG_LEVEL=INFO

# ── Langfuse ─────────────────────────────────────────────────────────────────
# Self hosted: So committing example keys for local testing. Replace with your own keys for production.
LANGFUSE_SECRET_KEY="sk-lf-1205fdff-cde4-409a-9b14-b9798dfa1ec0"
LANGFUSE_PUBLIC_KEY="pk-lf-77cf9a70-8fe4-4d9e-9cde-3d8aab018b72"
LANGFUSE_BASE_URL="http://localhost:3000"