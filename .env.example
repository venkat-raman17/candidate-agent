MCP_SERVER_URL=http://localhost:8081/mcp
MCP_CONNECT_TIMEOUT=30

# ── LLM: Anthropic (default) ─────────────────────────────────────────────────
# Required when LOCAL_LLM=false
ANTHROPIC_API_KEY=sk-ant-...
LLM_MODEL=claude-sonnet-4-6
LLM_TEMPERATURE=0.0

# ── LLM: Local (Ollama / LM Studio / vLLM) ───────────────────────────────────
# Set LOCAL_LLM=true to use a local OpenAI-compatible server instead of Anthropic.
# ANTHROPIC_API_KEY is not required in this mode.
#
# Ollama (default):
#   1. Install: https://ollama.com
#   2. Pull a model: ollama pull llama3.2
#   3. Set the vars below and LOCAL_LLM=true
#
# LM Studio: change LOCAL_LLM_BASE_URL to http://localhost:1234/v1
# vLLM:      change LOCAL_LLM_BASE_URL to http://localhost:8080/v1
LOCAL_LLM=false
LOCAL_LLM_BASE_URL=http://localhost:11434/v1
LOCAL_LLM_MODEL=llama3.2
LOCAL_LLM_API_KEY=ollama

# ── Server ────────────────────────────────────────────────────────────────────
APP_HOST=0.0.0.0
APP_PORT=8000
LOG_LEVEL=INFO
